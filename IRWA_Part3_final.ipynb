{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ariadna Gonzalez, JÃºlia Dalmau i Mireia Cuenca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import string\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Fri Sep 30 18:38:31 +0000 2022\", \"id\": 1575918028717707265, \"id_str\": \"1575918028717707265\", \"full_text\": \"Together we have raised over $20,000 in less than 24 hours for #HurricaneIan relief. \\n\\nPlease stop by the @BigManBigHeart_ tailgate tomorrow to donate in person. My family &amp; team will be at Tent #2 on the Legacy Walk near Gate K! \\n\\n#HurricaneRelief #NoleFam https://t.co/ikmWOP0bR0\", \"truncated\": false, \"display_text_range\": [0, 261], \"entities\": {\"hashtags\": [{\"text\": \"HurricaneIan\", \"indices\": [63, 76]}, {\"text\": \"HurricaneRelief\", \"indices\": [236, 252]}, {\"text\": \"NoleFam\", \"indices\": [253, 261]}], \"symbols\": [], \"user_mentions\": [{\"screen_name\": \"BigManBigHeart_\", \"name\": \"Big Man Big Heart\", \"id\": 1428430898609938433, \"id_str\": \"1428430898609938433\", \"indices\": [106, 122]}], \"urls\": [{\"url\": \"https://t.co/ikmWOP0bR0\", \"expanded_url\": \"https://twitter.com/gibbonsdillan/status/1575538547750162432\", \"display_url\": \"twitter.com/gibbonsdillan/\\u2026\", \"indices\": [262, 285]}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}, \"source\": \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\", \"in_reply_to_status_id\": null, \"in_reply_to_status_id_str\": null, \"in_reply_to_user_id\": null, \"in_reply_to_user_id_str\": null, \"in_reply_to_screen_name\": null, \"user\": {\"id\": 2534429279, \"id_str\": \"2534429279\", \"name\": \"Dillan R. Gibbons\", \"screen_name\": \"GibbonsDillan\", \"location\": \"USA\", \"description\": \"| St.Petersburg, FL | FSU Football | ND Football |       FSU MBA \\u201822 | ND Grad \\u201821 | CEO @BigManBigHeart_ | GoFundMe Athlete |\", \"url\": \"https://t.co/REbq6iCN2G\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"https://t.co/REbq6iCN2G\", \"expanded_url\": \"http://bigmanbigheart.com\", \"display_url\": \"bigmanbigheart.com\", \"indices\": [0, 23]}]}, \"description\": {\"urls\": []}}, \"protected\": false, \"followers_count\": 6106, \"friends_count\": 1513, \"listed_count\": 37, \"created_at\": \"Wed May 07 21:35:23 +0000 2014\", \"favourites_count\": 7674, \"utc_offset\": null, \"time_zone\": null, \"geo_enabled\": true, \"verified\": true, \"statuses_count\": 4807, \"lang\": null, \"contributors_enabled\": false, \"is_translator\": false, \"is_translation_enabled\": false, \"profile_background_color\": \"C0DEED\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_tile\": false, \"profile_image_url\": \"http://pbs.twimg.com/profile_images/1393414329551175683/yAaFKPs6_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/1393414329551175683/yAaFKPs6_normal.jpg\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/2534429279/1625196799\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"has_extended_profile\": true, \"default_profile\": true, \"default_profile_image\": false, \"following\": false, \"follow_request_sent\": false, \"notifications\": false, \"translator_type\": \"none\", \"withheld_in_countries\": []}, \"geo\": null, \"coordinates\": null, \"place\": {\"id\": \"ecbe2aea853af44e\", \"url\": \"https://api.twitter.com/1.1/geo/id/ecbe2aea853af44e.json\", \"place_type\": \"city\", \"name\": \"Tallahassee\", \"full_name\": \"Tallahassee, FL\", \"country_code\": \"US\", \"country\": \"United States\", \"contained_within\": [], \"bounding_box\": {\"type\": \"Polygon\", \"coordinates\": [[[-84.386548, 30.365093], [-84.1458, 30.365093], [-84.1458, 30.587338], [-84.386548, 30.587338]]]}, \"attributes\": {}}, \"contributors\": null, \"is_quote_status\": true, \"quoted_status_id\": 1575538547750162432, \"quoted_status_id_str\": \"1575538547750162432\", \"quoted_status\": {\"created_at\": \"Thu Sep 29 17:30:36 +0000 2022\", \"id\": 1575538547750162432, \"id_str\": \"1575538547750162432\", \"full_text\": \"We\\u2019re all we got, we\\u2019re all we need!\\n\\nWe don\\u2019t yet know the extent of the devastation from #HurricaneIan, but I know there will be countless needs. \\n\\nI\\u2019m teaming up with @jordantrav13 &amp; @Malakai_Menzer to set up a fund for hurricane relief/restoration:\\n\\nhttps://t.co/OpeNOZwpHH\", \"truncated\": false, \"display_text_range\": [0, 281], \"entities\": {\"hashtags\": [{\"text\": \"HurricaneIan\", \"indices\": [91, 104]}], \"symbols\": [], \"user_mentions\": [{\"screen_name\": \"jordantrav13\", \"name\": \"Jordan Travis\", \"id\": 1381969298172825601, \"id_str\": \"1381969298172825601\", \"indices\": [170, 183]}, {\"screen_name\": \"Malakai_Menzer\", \"name\": \"Malakai Jay Menzer\", \"id\": 927243280886255617, \"id_str\": \"927243280886255617\", \"indices\": [190, 205]}], \"urls\": [{\"url\": \"https://t.co/OpeNOZwpHH\", \"expanded_url\": \"https://gofund.me/2003a363\", \"display_url\": \"gofund.me/2003a363\", \"indices\": [258, 281]}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}, \"source\": \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\", \"in_reply_to_status_id\": null, \"in_reply_to_status_id_str\": null, \"in_reply_to_user_id\": null, \"in_reply_to_user_id_str\": null, \"in_reply_to_screen_name\": null, \"user\": {\"id\": 2534429279, \"id_str\": \"2534429279\", \"name\": \"Dillan R. Gibbons\", \"screen_name\": \"GibbonsDillan\", \"location\": \"USA\", \"description\": \"| St.Petersburg, FL | FSU Football | ND Football |       FSU MBA \\u201822 | ND Grad \\u201821 | CEO @BigManBigHeart_ | GoFundMe Athlete |\", \"url\": \"https://t.co/REbq6iCN2G\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"https://t.co/REbq6iCN2G\", \"expanded_url\": \"http://bigmanbigheart.com\", \"display_url\": \"bigmanbigheart.com\", \"indices\": [0, 23]}]}, \"description\": {\"urls\": []}}, \"protected\": false, \"followers_count\": 6106, \"friends_count\": 1513, \"listed_count\": 37, \"created_at\": \"Wed May 07 21:35:23 +0000 2014\", \"favourites_count\": 7674, \"utc_offset\": null, \"time_zone\": null, \"geo_enabled\": true, \"verified\": true, \"statuses_count\": 4807, \"lang\": null, \"contributors_enabled\": false, \"is_translator\": false, \"is_translation_enabled\": false, \"profile_background_color\": \"C0DEED\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_tile\": false, \"profile_image_url\": \"http://pbs.twimg.com/profile_images/1393414329551175683/yAaFKPs6_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/1393414329551175683/yAaFKPs6_normal.jpg\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/2534429279/1625196799\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"has_extended_profile\": true, \"default_profile\": true, \"default_profile_image\": false, \"following\": false, \"follow_request_sent\": false, \"notifications\": false, \"translator_type\": \"none\", \"withheld_in_countries\": []}, \"geo\": null, \"coordinates\": null, \"place\": {\"id\": \"ecbe2aea853af44e\", \"url\": \"https://api.twitter.com/1.1/geo/id/ecbe2aea853af44e.json\", \"place_type\": \"city\", \"name\": \"Tallahassee\", \"full_name\": \"Tallahassee, FL\", \"country_code\": \"US\", \"country\": \"United States\", \"contained_within\": [], \"bounding_box\": {\"type\": \"Polygon\", \"coordinates\": [[[-84.386548, 30.365093], [-84.1458, 30.365093], [-84.1458, 30.587338], [-84.386548, 30.587338]]]}, \"attributes\": {}}, \"contributors\": null, \"is_quote_status\": false, \"retweet_count\": 209, \"favorite_count\": 723, \"favorited\": false, \"retweeted\": false, \"possibly_sensitive\": false, \"lang\": \"en\"}, \"retweet_count\": 2, \"favorite_count\": 1, \"favorited\": false, \"retweeted\": false, \"possibly_sensitive\": false, \"lang\": \"en\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load data into memory\n",
    "\n",
    "docs_path = 'C:/Users/judal/Downloads/tw_hurricane_data.json' #path of tw_hurricane_data.json\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "    \n",
    "#Print lines[0] to see the structure of the data \n",
    "\n",
    "print(lines[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1575918028717707265"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform data into a dict to make easier the code\n",
    "\n",
    "datos_diccionario = [json.loads(line) for line in lines]\n",
    "\n",
    "#Now if we want to access to the screen_name of a user we colud do it like this\n",
    "\n",
    "datos_diccionario[12]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    #Transform in lowercase\n",
    "    line=  line.lower()\n",
    "    \n",
    "    #Removing punctuation marks\n",
    "    line= line.translate(str.maketrans('', '', string.punctuation)) \n",
    "    \n",
    "    #Tokenize the text to get a list of terms\n",
    "    line=  line.split(\" \")\n",
    "    \n",
    "    #Removing the stopwords\n",
    "    line=[x for x in line if x not in stop_words]\n",
    "    \n",
    "    #Perform stemming\n",
    "    line=[stemmer.stem(x) for x in line]\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(lines, num_documents):\n",
    "    \n",
    "    index = defaultdict(list)\n",
    "    title_index = defaultdict(float)\n",
    "    idf = defaultdict(float)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in documents\n",
    "    df = defaultdict(int)  #document frequencies of terms in the corpus\n",
    "    \n",
    "    for line in lines:\n",
    "    \n",
    "        terms = build_terms(line['full_text'])\n",
    "        \n",
    "        page_id = line['id']\n",
    "        title = line['id_str']\n",
    "\n",
    "        \n",
    "        # The final output must return Tweet, Username, Date, Hashtags, Likes, Retweets and Url\n",
    "        args = [line[\"id_str\"], line['user']['screen_name'], line['created_at'], line['entities']['hashtags'], line['user']['favourites_count'], line['retweet_count'], line['entities']['urls'],line['full_text']]\n",
    "        \n",
    "        title_index[title]=args\n",
    "        \n",
    "\n",
    "        positions_term_in_doc = {}\n",
    "        for position, term in enumerate(terms):\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                positions_term_in_doc[term].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                positions_term_in_doc[term] = [position]\n",
    "                \n",
    "        norm = 0\n",
    "        for term, posting in positions_term_in_doc.items():\n",
    "            norm += len(positions_term_in_doc[term]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "        \n",
    "        for term, posting in positions_term_in_doc.items():\n",
    "            tf[term].append(np.round(len(positions_term_in_doc[term]) / norm, 4))\n",
    "            df[term] += 1\n",
    "        \n",
    "        for term in terms:\n",
    "            index[term].append(title)\n",
    "            \n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents / df[term])), 4)\n",
    "            \n",
    "                    \n",
    "    return index, tf, df, idf, title_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TF-IDF + cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_tf_idf(terms, docs, index, idf, tf, title_index):\n",
    "    \n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms))\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) \n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        query_vector[termIndex] = query_terms_count[term] / query_norm * idf[term]\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, doc in enumerate(index[term]): \n",
    "            if doc in docs and len(tf[term])>doc_index:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term] \n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    \n",
    "    doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    result_scores= [x[0] for x in doc_scores]\n",
    "    \n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        #query = input()\n",
    "        #docs = search_tf_idf(query, index)\n",
    "    return result_scores,result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "   \n",
    "    query = build_terms(query)\n",
    "    docs = []\n",
    "    for term in query:\n",
    "        try:\n",
    "            #Term is in the index\n",
    "            keys = [i for i in index.keys()]\n",
    "            term_docs = [index[t] for t in keys if t==term]\n",
    "            docs=term_docs[0]\n",
    "            \n",
    "        except:\n",
    "            #Term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    scores_docs,ranked_docs = rank_documents_tf_idf(query, docs, index, idf, tf, title_index)\n",
    "    return scores_docs,ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_bm25(terms, docs, index, df, tf, title_index):\n",
    "    \n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms))\n",
    "    query_vector = [0] * len(terms)\n",
    "    N=len(docs)\n",
    "    k1=random.uniform(1.2,2)\n",
    "    b=0.75\n",
    "    RSV=dict()\n",
    "    lenghts=[]\n",
    "    longitud=len(terms)\n",
    "    tf_=np.zeros((longitud,N))\n",
    "    \n",
    "    for ids in docs:\n",
    "        tweet=title_index[ids][7]\n",
    "        terms_=build_terms(tweet)\n",
    "        lenght=len(terms_)\n",
    "        lenghts.append(lenght)\n",
    "               \n",
    "    for i in range(len(terms)):\n",
    "        for position,tweet in enumerate(docs):\n",
    "            count=0\n",
    "            terms_=build_terms(title_index[tweet][7])\n",
    "            for j in range(len(terms_)):\n",
    "                if terms[i]==terms_[j]:\n",
    "                    count+=1\n",
    "            tf_[i][position]=count\n",
    "                \n",
    "    Lave=np.mean(lenghts)\n",
    "    for position,tweet in enumerate(docs):\n",
    "        sumatorio=0\n",
    "        for i in range(len(terms)):\n",
    "            x=math.log((N/df[terms[i]]),2)\n",
    "            xx=tf_[i][position]\n",
    "            y=(k1+1)*xx\n",
    "            yy=lenghts[position]\n",
    "            z=k1*((1-b)+b*(yy/Lave)+xx)\n",
    "            sumatorio+=abs(x*(y/z))\n",
    "        RSV[tweet]=sumatorio\n",
    "\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    RSV_sort=dict(sorted(RSV.items(),key=lambda item:item[1],reverse=True))\n",
    "    result_docs = [x for x,y in RSV_sort.items()]\n",
    "    result_scores=[y for x,y in RSV_sort.items()]\n",
    "    \n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        \n",
    "    return result_scores,result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bm25(query, index):\n",
    "   \n",
    "    query = build_terms(query)\n",
    "    docs = []\n",
    "    for term in query:\n",
    "        try:\n",
    "            #Term is in the index\n",
    "            keys = [i for i in index.keys()]\n",
    "            term_docs = [index[t] for t in keys if t==term]\n",
    "            docs=term_docs[0]\n",
    "            \n",
    "        except:\n",
    "            #Term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    scores_docs,ranked_docs = rank_documents_bm25(query, docs, index, idf, tf, title_index)\n",
    "    return scores_docs,ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Our score + cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_our_score(terms, docs, index, idf, tf, title_index): \n",
    "    \n",
    "    sumatori=0\n",
    "    sumatori2=0\n",
    "    for doc in docs: \n",
    "        sumatori+=title_index[doc][4]\n",
    "        sumatori2+=title_index[doc][5]\n",
    "    if sumatori2==0: \n",
    "        sumatori2=1\n",
    "    if sumatori==0:\n",
    "        sumatori=1\n",
    "\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms))\n",
    "    query_vector = [0] * len(terms)\n",
    "    \n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "\n",
    "    for termIndex, term in enumerate(terms):\n",
    "        if term not in index:\n",
    "            continue\n",
    "        query_vector[termIndex] = query_terms_count[term] / query_norm * idf[term]\n",
    "        # Generate doc_vectors for matching docs\n",
    "        \n",
    "        for doc_index, doc in enumerate(index[term]): \n",
    "            if doc in docs and len(tf[term])>doc_index:\n",
    "                doc_vectors[doc][termIndex] = (title_index[doc][4]/sumatori)* (title_index[doc][5]/sumatori2)\n",
    "\n",
    "    # Calculate the score of each doc\n",
    "    doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    result_scores= [x[0] for x in doc_scores]\n",
    "    \n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        #query = input()\n",
    "        #docs = search_tf_idf(query, index)\n",
    "    return result_scores,result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_our_score(query, index):\n",
    "    query = build_terms(query)\n",
    "    docs = []\n",
    "    for term in query:\n",
    "        try:\n",
    "            #Term is in the index\n",
    "            keys = [i for i in index.keys()]\n",
    "            term_docs = [index[t] for t in keys if t==term]\n",
    "            docs=term_docs[0]\n",
    "        except:\n",
    "            #Term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    scores_docs,ranked_docs = rank_documents_our_score(query, docs, index, idf, tf, title_index)\n",
    "    return scores_docs,ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, tf, df, idf, title_index = create_index(datos_diccionario, len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 5 for the searched query:\n",
      "\n",
      "Tweet_id:  1575878436207443969  Username:  WorkingatDuke  Date:  Fri Sep 30 16:01:12 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [27, 40]}]  Likes:  3918  Url:  0\n",
      "Tweet_id:  1575914809383583744  Username:  wluera  Date:  Fri Sep 30 18:25:44 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [105, 118]}]  Likes:  7766  Url:  0\n",
      "Tweet_id:  1575900541221146625  Username:  twinmetalhen54  Date:  Fri Sep 30 17:29:02 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [113, 126]}, {'text': 'GSM', 'indices': [128, 132]}, {'text': 'Science', 'indices': [134, 142]}, {'text': 'Truth', 'indices': [144, 150]}]  Likes:  19694  Url:  0\n",
      "Tweet_id:  1575859051233038341  Username:  LexRich5Schools  Date:  Fri Sep 30 14:44:10 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [52, 65]}, {'text': 'D5Reads365', 'indices': [227, 238]}, {'text': 'OurD5Story', 'indices': [239, 250]}]  Likes:  10893  Url:  0\n",
      "Tweet_id:  1575889117942104065  Username:  GVWire  Date:  Fri Sep 30 16:43:38 +0000 2022  Hashtags:  [{'text': 'GVWire', 'indices': [70, 77]}, {'text': 'News', 'indices': [78, 83]}, {'text': 'Politics', 'indices': [84, 93]}, {'text': 'Weather', 'indices': [94, 102]}, {'text': 'Climate', 'indices': [103, 111]}, {'text': 'Environment', 'indices': [112, 124]}, {'text': 'Science', 'indices': [125, 133]}, {'text': 'Floods', 'indices': [134, 141]}, {'text': 'rain', 'indices': [142, 147]}, {'text': 'Hurricane', 'indices': [148, 158]}, {'text': 'HurricaneIan', 'indices': [159, 172]}, {'text': 'Carolina', 'indices': [173, 182]}, {'text': 'Florida', 'indices': [183, 191]}]  Likes:  898  Url:  0\n"
     ]
    }
   ],
   "source": [
    "#QUERIES\n",
    "\n",
    "query = \"Computer Science\"\n",
    "scores,docs = search_bm25(query, index)\n",
    "top = 10\n",
    "\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 3 for the searched query:\n",
      "\n",
      "Tweet_id:  1575887728964534273  Username:  pettigrewmed  Date:  Fri Sep 30 16:38:07 +0000 2022  Hashtags:  [{'text': 'hurricaneian', 'indices': [22, 35]}]  Likes:  31  Url:  0\n",
      "Tweet_id:  1575859537738838016  Username:  craigtimes  Date:  Fri Sep 30 14:46:06 +0000 2022  Hashtags:  [{'text': 'Florida', 'indices': [0, 8]}, {'text': 'HurricaneIan', 'indices': [112, 125]}]  Likes:  230080  Url:  11\n",
      "Tweet_id:  1575864352837701635  Username:  savcandy  Date:  Fri Sep 30 15:05:14 +0000 2022  Hashtags:  [{'text': 'savannahcandykitchen', 'indices': [238, 259]}, {'text': 'hurricaneian', 'indices': [260, 273]}]  Likes:  2288  Url:  1\n"
     ]
    }
   ],
   "source": [
    "query = \"instagram\"\n",
    "scores,docs = search_bm25(query, index)\n",
    "top = 10\n",
    "\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 2 for the searched query:\n",
      "\n",
      "Tweet_id:  1575905732649689089  Username:  spinning_will  Date:  Fri Sep 30 17:49:40 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [31, 44]}, {'text': 'FJB', 'indices': [61, 65]}]  Likes:  107663  Url:  0\n",
      "Tweet_id:  1575901730730283014  Username:  JTTmemes  Date:  Fri Sep 30 17:33:46 +0000 2022  Hashtags:  [{'text': 'TuaTagovailoa', 'indices': [182, 196]}, {'text': 'HurricaneIan', 'indices': [197, 210]}, {'text': 'coronavirus', 'indices': [211, 223]}, {'text': 'vaccine', 'indices': [224, 232]}]  Likes:  3664  Url:  0\n"
     ]
    }
   ],
   "source": [
    "query = \"vaccine\"\n",
    "scores,docs = search_bm25(query, index)\n",
    "top = 10\n",
    "\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 4 for the searched query:\n",
      "\n",
      "Tweet_id:  1575886977764728838  Username:  RamblingMyMind  Date:  Fri Sep 30 16:35:08 +0000 2022  Hashtags:  [{'text': 'COVID', 'indices': [92, 98]}, {'text': 'hurricaneian', 'indices': [131, 144]}, {'text': 'Hurricane', 'indices': [145, 155]}, {'text': 'JustBecauseYouCantSeeIt', 'indices': [156, 180]}]  Likes:  38047  Url:  1\n",
      "Tweet_id:  1575871626540818432  Username:  AMDPU22  Date:  Fri Sep 30 15:34:08 +0000 2022  Hashtags:  [{'text': 'MentalHealth', 'indices': [0, 13]}, {'text': 'COVID', 'indices': [55, 61]}, {'text': 'HurricaneIan', 'indices': [66, 79]}, {'text': 'DePaulSMN', 'indices': [151, 161]}]  Likes:  27  Url:  0\n",
      "Tweet_id:  1575865182944894978  Username:  FraserFaithful  Date:  Fri Sep 30 15:08:32 +0000 2022  Hashtags:  [{'text': 'JunkScience', 'indices': [0, 12]}, {'text': 'GlobalWarming', 'indices': [38, 52]}, {'text': 'HurricaneIan', 'indices': [71, 84]}, {'text': 'Covid', 'indices': [132, 138]}, {'text': 'FakeNews', 'indices': [192, 201]}]  Likes:  1717  Url:  2\n",
      "Tweet_id:  1575860573795127296  Username:  holdsworth353  Date:  Fri Sep 30 14:50:13 +0000 2022  Hashtags:  [{'text': 'Florida', 'indices': [37, 45]}, {'text': 'Covid', 'indices': [75, 81]}, {'text': 'HurricaneIan', 'indices': [232, 245]}, {'text': 'Cleanup', 'indices': [246, 254]}, {'text': 'MyStory', 'indices': [255, 263]}]  Likes:  4535  Url:  1\n"
     ]
    }
   ],
   "source": [
    "query = \"covid\"\n",
    "scores,docs = search_bm25(query, index)\n",
    "top = 10\n",
    "\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 263 for the searched query:\n",
      "\n",
      "Tweet_id:  1575914959091163136  Username:  sfdb  Date:  Fri Sep 30 18:26:19 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [0, 13]}]  Likes:  18768  Url:  0\n",
      "Tweet_id:  1575875671385071618  Username:  JoshFitzWx  Date:  Fri Sep 30 15:50:13 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [3, 16]}]  Likes:  26  Url:  1\n",
      "Tweet_id:  1575913904143626240  Username:  NicholeDWBZ  Date:  Fri Sep 30 18:22:08 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [5, 18]}]  Likes:  6617  Url:  0\n",
      "Tweet_id:  1575914903806042112  Username:  WIONews  Date:  Fri Sep 30 18:26:06 +0000 2022  Hashtags:  [{'text': 'BREAKING', 'indices': [0, 9]}, {'text': 'HurricaneIan', 'indices': [12, 25]}]  Likes:  7271  Url:  1\n",
      "Tweet_id:  1575910357293764608  Username:  AlecSilvaWX  Date:  Fri Sep 30 18:08:02 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [0, 13]}]  Likes:  6259  Url:  0\n",
      "Tweet_id:  1575873971827798016  Username:  bettycjung  Date:  Fri Sep 30 15:43:27 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [24, 37]}]  Likes:  2928  Url:  0\n",
      "Tweet_id:  1575870287056248833  Username:  dleegilb  Date:  Fri Sep 30 15:28:49 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [80, 93]}]  Likes:  6471  Url:  0\n",
      "Tweet_id:  1575898335281831936  Username:  digitaljournal  Date:  Fri Sep 30 17:20:16 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [0, 13]}]  Likes:  27900  Url:  0\n",
      "Tweet_id:  1575900845102841857  Username:  ABC7NY  Date:  Fri Sep 30 17:30:14 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [27, 40]}]  Likes:  7742  Url:  1\n",
      "Tweet_id:  1575881179907112961  Username:  AlecSilvaWX  Date:  Fri Sep 30 16:12:06 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [0, 13]}]  Likes:  6259  Url:  0\n"
     ]
    }
   ],
   "source": [
    "query = \"Landfall in South Carolina\"\n",
    "scores, docs = search_bm25(query, index)\n",
    "top = 10\n",
    "\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 5 for the searched query:\n",
      "\n",
      "Tweet_id:  1575914809383583744  Username:  wluera  Date:  Fri Sep 30 18:25:44 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [105, 118]}]  Likes:  7766  Url:  0\n",
      "Tweet_id:  1575900541221146625  Username:  twinmetalhen54  Date:  Fri Sep 30 17:29:02 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [113, 126]}, {'text': 'GSM', 'indices': [128, 132]}, {'text': 'Science', 'indices': [134, 142]}, {'text': 'Truth', 'indices': [144, 150]}]  Likes:  19694  Url:  0\n",
      "Tweet_id:  1575889117942104065  Username:  GVWire  Date:  Fri Sep 30 16:43:38 +0000 2022  Hashtags:  [{'text': 'GVWire', 'indices': [70, 77]}, {'text': 'News', 'indices': [78, 83]}, {'text': 'Politics', 'indices': [84, 93]}, {'text': 'Weather', 'indices': [94, 102]}, {'text': 'Climate', 'indices': [103, 111]}, {'text': 'Environment', 'indices': [112, 124]}, {'text': 'Science', 'indices': [125, 133]}, {'text': 'Floods', 'indices': [134, 141]}, {'text': 'rain', 'indices': [142, 147]}, {'text': 'Hurricane', 'indices': [148, 158]}, {'text': 'HurricaneIan', 'indices': [159, 172]}, {'text': 'Carolina', 'indices': [173, 182]}, {'text': 'Florida', 'indices': [183, 191]}]  Likes:  898  Url:  0\n",
      "Tweet_id:  1575878436207443969  Username:  WorkingatDuke  Date:  Fri Sep 30 16:01:12 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [27, 40]}]  Likes:  3918  Url:  0\n",
      "Tweet_id:  1575859051233038341  Username:  LexRich5Schools  Date:  Fri Sep 30 14:44:10 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [52, 65]}, {'text': 'D5Reads365', 'indices': [227, 238]}, {'text': 'OurD5Story', 'indices': [239, 250]}]  Likes:  10893  Url:  0\n"
     ]
    }
   ],
   "source": [
    "#model our_search_score\n",
    "query = \"Computer Science\"\n",
    "scores,docs = search_our_score(query, index)\n",
    "top = 10\n",
    "\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 255 for the searched query:\n",
      "\n",
      "Tweet_id:  1575873317483036681  Username:  capitalweather  Date:  Fri Sep 30 15:40:51 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [14, 27]}]  Likes:  44216  Url:  42\n",
      "Tweet_id:  1575909527924490241  Username:  WeatherNation  Date:  Fri Sep 30 18:04:45 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [33, 46]}, {'text': 'SCwx', 'indices': [243, 248]}]  Likes:  82674  Url:  21\n",
      "Tweet_id:  1575861142211235842  Username:  HananyaNaftali  Date:  Fri Sep 30 14:52:29 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [187, 200]}]  Likes:  35667  Url:  46\n",
      "Tweet_id:  1575860260614832128  Username:  capitalweather  Date:  Fri Sep 30 14:48:58 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [63, 76]}]  Likes:  44216  Url:  17\n",
      "Tweet_id:  1575865415225729024  Username:  B_Carp01  Date:  Fri Sep 30 15:09:27 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [13, 26]}, {'text': 'SCwx', 'indices': [275, 280]}]  Likes:  121359  Url:  3\n",
      "Tweet_id:  1575874616454586369  Username:  DemsKeys  Date:  Fri Sep 30 15:46:01 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [81, 94]}, {'text': 'DemCastFL', 'indices': [256, 266]}, {'text': 'OVFL', 'indices': [267, 272]}, {'text': 'ONEV1', 'indices': [273, 279]}]  Likes:  79959  Url:  13\n",
      "Tweet_id:  1575881160650956800  Username:  pdesiperez  Date:  Fri Sep 30 16:12:01 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [37, 50]}, {'text': 'NC', 'indices': [200, 203]}]  Likes:  26587  Url:  7\n",
      "Tweet_id:  1575891910291771392  Username:  SCEMD  Date:  Fri Sep 30 16:54:44 +0000 2022  Hashtags:  [{'text': 'scwx', 'indices': [123, 128]}, {'text': 'HurricaneIan', 'indices': [129, 142]}, {'text': 'sctweets', 'indices': [143, 152]}]  Likes:  6988  Url:  25\n",
      "Tweet_id:  1575914423402766336  Username:  accuweather  Date:  Fri Sep 30 18:24:12 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [10, 23]}]  Likes:  12941  Url:  7\n",
      "Tweet_id:  1575901072576626688  Username:  BobbiStorm  Date:  Fri Sep 30 17:31:09 +0000 2022  Hashtags:  [{'text': 'ian', 'indices': [0, 4]}, {'text': 'hurricane', 'indices': [21, 31]}, {'text': 'landfall', 'indices': [46, 55]}, {'text': 'hurricaneian', 'indices': [57, 70]}, {'text': 'FL', 'indices': [81, 84]}, {'text': 'Carolinas', 'indices': [85, 95]}]  Likes:  39720  Url:  3\n"
     ]
    }
   ],
   "source": [
    "query = \"Landfall in South Carolina\"\n",
    "scores, docs = search_our_score(query, index)\n",
    "top = 10\n",
    "\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 4 for the searched query:\n",
      "\n",
      "Tweet_id:  1575886977764728838  Username:  RamblingMyMind  Date:  Fri Sep 30 16:35:08 +0000 2022  Hashtags:  [{'text': 'COVID', 'indices': [92, 98]}, {'text': 'hurricaneian', 'indices': [131, 144]}, {'text': 'Hurricane', 'indices': [145, 155]}, {'text': 'JustBecauseYouCantSeeIt', 'indices': [156, 180]}]  Likes:  38047  Url:  1\n",
      "Tweet_id:  1575860573795127296  Username:  holdsworth353  Date:  Fri Sep 30 14:50:13 +0000 2022  Hashtags:  [{'text': 'Florida', 'indices': [37, 45]}, {'text': 'Covid', 'indices': [75, 81]}, {'text': 'HurricaneIan', 'indices': [232, 245]}, {'text': 'Cleanup', 'indices': [246, 254]}, {'text': 'MyStory', 'indices': [255, 263]}]  Likes:  4535  Url:  1\n",
      "Tweet_id:  1575865182944894978  Username:  FraserFaithful  Date:  Fri Sep 30 15:08:32 +0000 2022  Hashtags:  [{'text': 'JunkScience', 'indices': [0, 12]}, {'text': 'GlobalWarming', 'indices': [38, 52]}, {'text': 'HurricaneIan', 'indices': [71, 84]}, {'text': 'Covid', 'indices': [132, 138]}, {'text': 'FakeNews', 'indices': [192, 201]}]  Likes:  1717  Url:  2\n",
      "Tweet_id:  1575871626540818432  Username:  AMDPU22  Date:  Fri Sep 30 15:34:08 +0000 2022  Hashtags:  [{'text': 'MentalHealth', 'indices': [0, 13]}, {'text': 'COVID', 'indices': [55, 61]}, {'text': 'HurricaneIan', 'indices': [66, 79]}, {'text': 'DePaulSMN', 'indices': [151, 161]}]  Likes:  27  Url:  0\n"
     ]
    }
   ],
   "source": [
    "query = \"covid\"\n",
    "scores,docs = search_our_score(query, index)\n",
    "top = 10\n",
    "\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 3 for the searched query:\n",
      "\n",
      "Tweet_id:  1575859537738838016  Username:  craigtimes  Date:  Fri Sep 30 14:46:06 +0000 2022  Hashtags:  [{'text': 'Florida', 'indices': [0, 8]}, {'text': 'HurricaneIan', 'indices': [112, 125]}]  Likes:  230080  Url:  11\n",
      "Tweet_id:  1575864352837701635  Username:  savcandy  Date:  Fri Sep 30 15:05:14 +0000 2022  Hashtags:  [{'text': 'savannahcandykitchen', 'indices': [238, 259]}, {'text': 'hurricaneian', 'indices': [260, 273]}]  Likes:  2288  Url:  1\n",
      "Tweet_id:  1575887728964534273  Username:  pettigrewmed  Date:  Fri Sep 30 16:38:07 +0000 2022  Hashtags:  [{'text': 'hurricaneian', 'indices': [22, 35]}]  Likes:  31  Url:  0\n"
     ]
    }
   ],
   "source": [
    "query = \"instagram\"\n",
    "scores,docs = search_our_score(query, index)\n",
    "top = 10\n",
    "\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 2 for the searched query:\n",
      "\n",
      "Tweet_id:  1575905732649689089  Username:  spinning_will  Date:  Fri Sep 30 17:49:40 +0000 2022  Hashtags:  [{'text': 'HurricaneIan', 'indices': [31, 44]}, {'text': 'FJB', 'indices': [61, 65]}]  Likes:  107663  Url:  0\n",
      "Tweet_id:  1575901730730283014  Username:  JTTmemes  Date:  Fri Sep 30 17:33:46 +0000 2022  Hashtags:  [{'text': 'TuaTagovailoa', 'indices': [182, 196]}, {'text': 'HurricaneIan', 'indices': [197, 210]}, {'text': 'coronavirus', 'indices': [211, 223]}, {'text': 'vaccine', 'indices': [224, 232]}]  Likes:  3664  Url:  0\n"
     ]
    }
   ],
   "source": [
    "query = \"vaccine\"\n",
    "scores,docs = search_our_score(query, index)\n",
    "top = 10\n",
    "\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top20_2vec(query, tweets, index, title_index, model):\n",
    "    terms={}\n",
    "    for ids in tweets:\n",
    "        term_tweet=build_terms(title_index[ids][7])\n",
    "        \n",
    "        wordspresent = [word for word in term_tweet if word in model.wv.index_to_key]\n",
    "        vector = np.mean(model.wv[wordspresent], axis=0)\n",
    "        terms[ids] = vector\n",
    "      \n",
    "    \n",
    "    \n",
    "    wordspresent = [word for word in query if word in model.wv.index_to_key]\n",
    "    queryVector =  np.mean(model.wv[wordspresent], axis=0)\n",
    "    \n",
    "    \n",
    "    #calculate cosine similarity  \n",
    "    tweetScores = [ [np.dot(curTweetVec, queryVector), tweet_id] for tweet_id, curTweetVec in terms.items() ]\n",
    "    tweetScores.sort(reverse=True)\n",
    "    resultTweets = [x[1] for x in tweetScores][:20]\n",
    "    resultScores = [x[0] for x in tweetScores][:20]\n",
    "    \n",
    "    return  resultScores, resultTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query,index,tweets,tf,idf,model,title_index):\n",
    "    \n",
    "    query= build_terms(query)\n",
    "    docs = []\n",
    "    for term in query:\n",
    "        try:\n",
    "            #Term is in the index\n",
    "            keys = [i for i in index.keys()]\n",
    "            term_docs = [index[t] for t in keys if t==term]\n",
    "            docs=term_docs[0]\n",
    "            \n",
    "        except:\n",
    "            #Term is not in index\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "    tweets = list(docs)\n",
    "    scores_docs,ranked_docs = top20_2vec(query, tweets, index, title_index,model)\n",
    "    return scores_docs,ranked_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "clean_tweets=[]\n",
    "for tweets in datos_diccionario:\n",
    "    terms=build_terms(tweets['full_text'])\n",
    "    clean_tweets.append(terms)\n",
    "    \n",
    "model = Word2Vec(clean_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Computer Science\"\n",
    "\n",
    "scores,word2vectop20 = search(query, index, datos_diccionario,tf,idf,model,title_index)\n",
    "\n",
    "print(\"\\n======================\\nTop 20 results for the searched query, using Word2Vec :\\n\")\n",
    "for d_id in word2vectop20[:top]:\n",
    "    print(d_id)\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Landfall in South Carolina\"\n",
    "scores,word2vectop20 = search(query, index, datos_diccionario,tf,idf,model,title_index)\n",
    "\n",
    "print(\"\\n======================\\nTop 20 results for the searched query, using Word2Vec :\\n\")\n",
    "for d_id in word2vectop20[:top]:\n",
    "    print(d_id)\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"university\"\n",
    "scores,word2vectop20 = search(query, index, datos_diccionario,tf,idf,model,title_index)\n",
    "\n",
    "print(\"\\n======================\\nTop 20 results for the searched query, using Word2Vec :\\n\")\n",
    "for d_id in word2vectop20[:top]:\n",
    "    print(d_id)\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"climate\"\n",
    "scores,word2vectop20 = search(query, index, datos_diccionario,tf,idf,model,title_index)\n",
    "\n",
    "print(\"\\n======================\\nTop 20 results for the searched query, using Word2Vec :\\n\")\n",
    "for d_id in word2vectop20[:top]:\n",
    "    print(d_id)\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"crisis\"\n",
    "scores,word2vectop20 = search(query, index, datos_diccionario,tf,idf,model,title_index)\n",
    "\n",
    "print(\"\\n======================\\nTop 20 results for the searched query, using Word2Vec :\\n\")\n",
    "for d_id in word2vectop20[:top]:\n",
    "    print(d_id)\n",
    "    print('Tweet_id: ', title_index[d_id][0], ' Username: ', title_index[d_id][1], ' Date: ', title_index[d_id][2], ' Hashtags: ', title_index[d_id][3], ' Likes: ', title_index[d_id][4], ' Url: ', title_index[d_id][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
